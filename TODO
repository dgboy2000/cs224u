Better coding:
    - Refactor 'kaggle.py' to be more general
    - Use constants throughout all code for the data files (stop using the strings)
    - Automatic creation of data/corpus.txt

Things to consider:

- Uni/bi-grams: 
    - Different corpus?
    - Different ordering of words for feature selection (i.e., instead of raw frequency)
    - Better filtering of non-relevant words? (i.e., get rid of stop words, etc.)
    - Check out better tokenization :: nltk.download() will get us nltk.data.load('tokenizers/punkt/english.pickle')
- How to build a more general model using all of the entry sets, and not just learning on each set?




Try modeling individual graders
Look at x/y plot of projected 1-dimensional feature vs grades for each grader
fix kaggle.py to be better
output ranked in order of how badly we predict.


Learning ideas:

TODO:
anneal eta down per cs246
take command line parameter options a la svm light
tune parameters
do I need to update w for even the pairs of points with zero slack?

Ideas:
cross validation framework to set SVM parameters. specify validation set, svm tries a few parameter settings and picks the best one on the validation data
non-linear kernel
numerical optimization of learning: save last 3 parameter vectors and objective scores, take a quadratic Newton-descent type step

